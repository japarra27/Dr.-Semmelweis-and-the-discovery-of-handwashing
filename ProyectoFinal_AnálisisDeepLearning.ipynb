{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProyectoFinal - AnálisisDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVW35W54t8p94mwYhryuRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/japarra27/Dr.-Semmelweis-and-the-discovery-of-handwashing/blob/master/ProyectoFinal_An%C3%A1lisisDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCJ2ifkG0K-V"
      },
      "source": [
        "![image](https://docs.google.com/uc?export=download&id=1NUy1Q-abpoV9XYK9qT9t8Mdhj3ZVlveO)\n",
        "\n",
        "# **Proyecto Final**\n",
        "\n",
        "* Steven Llerena - 202010212 \n",
        "* Jaime Andrés Parra - 202107161\n",
        "* Jesús Ramírez - 201015827\n",
        "\n",
        "## **Contenido**\n",
        "1. [**Problema**](#id1)\n",
        "2. [**Instalando e Importando las librerías necesarias para el laboratorio**](#id2)\n",
        "3. [**Cargue de datos**](#id3)\n",
        "4. [**Entendimiento del dataset**](#id4)\n",
        "5. [**Modelamiento**](#id5)\n",
        "6. [**Estimación de métricas**](#id6)\n",
        "7. [**Conclusiones**](#id7)\n",
        "\n",
        "## **Problema**<a name=\"id1\"></a>\n",
        "\n",
        "- <p align = \"justify\">Actualmente el tiempo utilizado en investigación de papers para saber si aportarán a la investigación es un proceso largo y en ocasiones poco fructifero, por lo tanto se va a realizar un modelo de resumen de texto utilizando transformers. El dataset a utilizar es el CORD19.:</p>\n",
        "\n",
        "> [Fuente de datos](https://www.semanticscholar.org/cord19/download)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3dJhYjOhcgY"
      },
      "source": [
        "https://colab.research.google.com/github/PubChimps/ibmvirtualmeetups/blob/master/5-12/meetup.ipynb#scrollTo=8-hqHk_49P6h\n",
        "https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb#scrollTo=xxLcdj2yvSU3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-OBxJHS43Rm"
      },
      "source": [
        "## **Instalando e Importando las librerías necesarias para el laboratorio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6k3f0tQ5KgK"
      },
      "source": [
        "%%capture\n",
        "# Configuración de utilidades colab\n",
        "!shred -u setup_colab_general.py\n",
        "!wget -q \"https://github.com/jpcano1/python_utils/raw/main/setup_colab_general.py\" -O setup_colab_general.py\n",
        "!pip install --progress-bar off -q tqdm==4.56.0\n",
        "!pip install datasets\n",
        "!pip install wandb\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip install SentencePiece\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzGtu5b840iK"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib_Qf6Ma5seR"
      },
      "source": [
        "%%capture\n",
        "# Importando las librerias a utilizar\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "#from nlp import load_metric\n",
        "import datasets\n",
        "from datasets import list_datasets, load_dataset, load_metric\n",
        "from pprint import pprint\n",
        "import wandb\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_BgITNz6ipK"
      },
      "source": [
        "## Login a wandb para autorizar el apikey\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZXMxTQ36_W9"
      },
      "source": [
        "# Login para empezar el monitoreo del modelo\n",
        "wandb_logger = WandbLogger(project='resumen-textos-transformers')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBQtrtbx9D1p"
      },
      "source": [
        "## **Cargue de datos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elQRSJ_Q9HVC"
      },
      "source": [
        "- <p align = \"justify\">Actualmente, el dataset de CORD19, bajo la administración del Instituto Allen puede ser descargado desde su página oficial de forma gratuita. Sin embargo, gracias al trabajo de Priya Dwivedi se cuenta con la libreria nlp, con la cual podemos hacer la descarga del dataset ya procesado y optimizado para los diferentes casos de uso que se quieren realizar.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyuIohr5X0O6"
      },
      "source": [
        "### **Atributos del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSLvheZoBnK6"
      },
      "source": [
        "# Atributos del dataset\n",
        "cord_dataset = list_datasets(with_details=True)[list_datasets().index('cord19')]\n",
        "pprint(cord_dataset.__dict__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eir9jA9WX5fj"
      },
      "source": [
        "### **Descarga del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VGEju_ZKEqF"
      },
      "source": [
        "# Descarga del datset\n",
        "dataset = load_dataset('cord19', \"fulltext\", data_dir='data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91UcFX9pQhTp"
      },
      "source": [
        "# Información del dataset\n",
        "pprint(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZb09Xc5V6CX"
      },
      "source": [
        "# Validación de las llaves del dataset\n",
        "dataset.keys(), dataset.get(\"train\")[0].keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4JJk4tEYCRs"
      },
      "source": [
        "### **Ejemplo de las características de un paper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMyRkJ0HW_oq"
      },
      "source": [
        "# Ejemplo de un título de un paper\n",
        "pprint(dataset.get(\"train\")[100].get('title'))\n",
        "print(\"\\n the len of the title is:\", len(dataset.get(\"train\")[100].get('title').split(\" \")), 'words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ySLySayUaeE"
      },
      "source": [
        "# Ejemplo de un texto completo\n",
        "pprint(dataset.get(\"train\")[100].get('fulltext'))\n",
        "print(\"\\n the len of the paper is:\", len(dataset.get(\"train\")[100].get('fulltext').split(\" \")), 'words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wuKER8UXjOe"
      },
      "source": [
        "# Ejemplo de un abstract\n",
        "pprint(dataset.get(\"train\")[100].get('abstract'))\n",
        "print(\"\\n the len of the paper is:\", len(dataset.get(\"train\")[100].get('abstract').split(\" \")), 'words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9MnaieVYVvs"
      },
      "source": [
        "## **Entendimiento del dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwEsH5jHYZNE"
      },
      "source": [
        "### **Estimación de la cantidad promedio de palabras en los papers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW0doccMYLig"
      },
      "source": [
        "muestra_dataset = dataset.get('train').select(list(range(0, 1000)))\n",
        "texto_len = []\n",
        "summary_len=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AEIhBn9Yl2I"
      },
      "source": [
        "for i in range(len(muestra_dataset)):\n",
        "    ejemplo = muestra_dataset[i]\n",
        "    texto_ejemplo = ejemplo.get('fulltext')\n",
        "    texto_ejemplo = texto_ejemplo.replace('\\n','')\n",
        "    texto_words = texto_ejemplo.split()\n",
        "    texto_len.append(len(texto_words))\n",
        "    summary_ejemplo = ejemplo['abstract']\n",
        "    summary_ejemplo = summary_ejemplo.replace('\\n','')\n",
        "    summary_words = summary_ejemplo.split()\n",
        "    summary_len.append(len(summary_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osQz_dnMZDUt"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(texto_len, bins=10)\n",
        "plt.title('Distribución de la cantidad de palabras texto completo - Primeros 1000 ejemplos')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwRVZUKBZVpG"
      },
      "source": [
        "plt.hist(summary_len)\n",
        "plt.title('Distribución de la cantidad de palabras abstract - Primeros 1000 ejemplos')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1ltL0X6Ze2O"
      },
      "source": [
        "print(\"Promedio palabras fulltext: \", sum(texto_len)/len(texto_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ZkvsYcZmQX"
      },
      "source": [
        "print(\"Promedio palabras abstract: \", sum(summary_len)/len(summary_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L27jk7kXZd7x"
      },
      "source": [
        "## **Modelamiento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIWvEf1sgFj4"
      },
      "source": [
        "# Eliminación de caracteres especiales\n",
        "CHARS = [\n",
        "    \"¦\",\n",
        "    \"§\",\n",
        "    \"¨\",\n",
        "    \"©\",\n",
        "    \"ª\",\n",
        "    \"«\",\n",
        "    \"®\",\n",
        "    \"¯\",\n",
        "    \"°\",\n",
        "    \"±\",\n",
        "    \"²\",\n",
        "    \"³\",\n",
        "    \"´\",\n",
        "    \"µ\",\n",
        "    \"¶\",\n",
        "    \"·\",\n",
        "    \"º\",\n",
        "    \"»\",\n",
        "    \"¼\",\n",
        "    \"½\",\n",
        "    \"¿\",\n",
        "    \"×\",\n",
        "    \"Ø\",\n",
        "    \"÷\",\n",
        "    \"ø\",\n",
        "    \"Ɵ\",\n",
        "    \"Ƶ\",\n",
        "    \"ǁ\",\n",
        "    \"ǆ\",\n",
        "    \"Ǉ\",\n",
        "    \"ǌ\",\n",
        "    \"ʹ\",\n",
        "    \"ʼ\",\n",
        "    \"ˆ\",\n",
        "    \"ˇ\",\n",
        "    \"À\",\n",
        "    \"Á\",\n",
        "    \"Â\",\n",
        "    \"Ã\",\n",
        "    \"Ä\",\n",
        "    \"Å\",\n",
        "    \"Ç\",\n",
        "    \"È\",\n",
        "    \"É\",\n",
        "    \"Ê\",\n",
        "    \"Í\",\n",
        "    \"Ð\",\n",
        "    \"Ñ\",\n",
        "    \"Ò\",\n",
        "    \"Ó\",\n",
        "    \"Ô\",\n",
        "    \"Õ\",\n",
        "    \"Ö\",\n",
        "    \"Ú\",\n",
        "    \"Û\",\n",
        "    \"Ü\",\n",
        "    \"Þ\",\n",
        "    \"ß\",\n",
        "    \"à\",\n",
        "    \"á\",\n",
        "    \"â\",\n",
        "    \"ã\",\n",
        "    \"ä\",\n",
        "    \"å\",\n",
        "    \"ç\",\n",
        "    \"è\",\n",
        "    \"é\",\n",
        "    \"ê\",\n",
        "    \"ë\",\n",
        "    \"ì\",\n",
        "    \"í\",\n",
        "    \"î\",\n",
        "    \"ï\",\n",
        "    \"ð\",\n",
        "    \"ñ\",\n",
        "    \"ò\",\n",
        "    \"ó\",\n",
        "    \"ô\",\n",
        "    \"õ\",\n",
        "    \"ö\",\n",
        "    \"ù\",\n",
        "    \"ú\",\n",
        "    \"û\",\n",
        "    \"ü\",\n",
        "    \"ý\",\n",
        "    \"þ\",\n",
        "    \"ÿ\",\n",
        "    \"ā\",\n",
        "    \"Ă\",\n",
        "    \"ą\",\n",
        "    \"Ć\",\n",
        "    \"ć\",\n",
        "    \"Č\",\n",
        "    \"č\",\n",
        "    \"ď\",\n",
        "    \"Đ\",\n",
        "    \"ē\",\n",
        "    \"ę\",\n",
        "    \"Ě\",\n",
        "    \"ě\",\n",
        "    \"Ğ\",\n",
        "    \"Ĩ\",\n",
        "    \"Į\",\n",
        "    \"ı\",\n",
        "    \"ĸ\",\n",
        "    \"Ĺ\",\n",
        "    \"ł\",\n",
        "    \"ń\",\n",
        "    \"Ň\",\n",
        "    \"Ō\",\n",
        "    \"ō\",\n",
        "    \"Ő\",\n",
        "    \"ő\",\n",
        "    \"Ś\",\n",
        "    \"ś\",\n",
        "    \"ŝ\",\n",
        "    \"ş\",\n",
        "    \"Š\",\n",
        "    \"š\",\n",
        "    \"Ŭ\",\n",
        "    \"ů\",\n",
        "    \"ŵ\",\n",
        "    \"Ŷ\",\n",
        "    \"ź\",\n",
        "    \"ż\",\n",
        "    \"Ž\",\n",
        "    \"ž\",\n",
        "    \"Ɖ\",\n",
        "    \"Ƌ\",\n",
        "    \"ƌ\",\n",
        "    \"Ɛ\",\n",
        "    \"ƚ\",\n",
        "    \"ǎ\",\n",
        "    \"ǐ\",\n",
        "    \"ǒ\",\n",
        "    \"ǔ\",\n",
        "    \"ǡ\",\n",
        "    \"ș\",\n",
        "    \"ɑ\",\n",
        "    \"ɛ\",\n",
        "    \"ɣ\",\n",
        "    \"ʋ\",\n",
        "    \"˘\",\n",
        "    \"˚\",\n",
        "    \"˛\",\n",
        "    \"˝\",\n",
        "    \"́\",\n",
        "    \"̇\",\n",
        "    \"͕\",\n",
        "    \"͖\",\n",
        "    \"͗\",\n",
        "    \"͘\",\n",
        "    \"ͬ\",\n",
        "    \"Ͳ\",\n",
        "    \"а\",\n",
        "    \"б\",\n",
        "    \"в\",\n",
        "    \"г\",\n",
        "    \"д\",\n",
        "    \"е\",\n",
        "    \"ж\",\n",
        "    \"з\",\n",
        "    \"и\",\n",
        "    \"й\",\n",
        "    \"к\",\n",
        "    \"л\",\n",
        "    \"м\",\n",
        "    \"н\",\n",
        "    \"о\",\n",
        "    \"п\",\n",
        "    \"р\",\n",
        "    \"с\",\n",
        "    \"т\",\n",
        "    \"у\",\n",
        "    \"ф\",\n",
        "    \"х\",\n",
        "    \"ц\",\n",
        "    \"ч\",\n",
        "    \"ш\",\n",
        "    \"щ\",\n",
        "    \"ы\",\n",
        "    \"ь\",\n",
        "    \"э\",\n",
        "    \"ю\",\n",
        "    \"я\",\n",
        "    \"ӧ\",\n",
        "    \"Յ\",\n",
        "    \"Ն\",\n",
        "    \"؉\",\n",
        "    \"؊\",\n",
        "    \"؋\",\n",
        "    \"،\",\n",
        "    \"؍\",\n",
        "    \"؎\",\n",
        "    \"ء\",\n",
        "    \"آ\",\n",
        "    \"أ\",\n",
        "    \"ؤ\",\n",
        "    \"إ\",\n",
        "    \"ئ\",\n",
        "    \"ا\",\n",
        "    \"ب\",\n",
        "    \"ة\",\n",
        "    \"ت\",\n",
        "    \"ث\",\n",
        "    \"ج\",\n",
        "    \"ح\",\n",
        "    \"خ\",\n",
        "    \"د\",\n",
        "    \"ذ\",\n",
        "    \"ر\",\n",
        "    \"ز\",\n",
        "    \"س\",\n",
        "    \"ش\",\n",
        "    \"ص\",\n",
        "    \"ض\",\n",
        "    \"ط\",\n",
        "    \"ظ\",\n",
        "    \"ع\",\n",
        "    \"غ\",\n",
        "    \"ف\",\n",
        "    \"ق\",\n",
        "    \"ك\",\n",
        "    \"ل\",\n",
        "    \"م\",\n",
        "    \"ن\",\n",
        "    \"ه\",\n",
        "    \"و\",\n",
        "    \"ى\",\n",
        "    \"ي\",\n",
        "    \"ً\",\n",
        "    \"ٌ\",\n",
        "    \"ٍ\",\n",
        "    \"َ\",\n",
        "    \"ُ\",\n",
        "    \"ِ\",\n",
        "    \"ّ\",\n",
        "    \"ْ\",\n",
        "    \"ܰ\",\n",
        "    \"ܴ\",\n",
        "    \"݅\",\n",
        "    \"݇\",\n",
        "    \"ݏ\",\n",
        "    \"ݑ\",\n",
        "    \"ݕ\",\n",
        "    \"ߚ\",\n",
        "    \"ߜ\",\n",
        "    \"ߤ\",\n",
        "    \"ߪ\",\n",
        "    \"ଝ\",\n",
        "    \"ଵ\",\n",
        "    \"ଶ\",\n",
        "    \"᭧\",\n",
        "    \"ᮊ\",\n",
        "    \"ᵒ\",\n",
        "    \"Ḡ\",\n",
        "    \"ỹ\",\n",
        "    \"‖\",\n",
        "    \"‚\",\n",
        "    \"†\",\n",
        "    \"‡\",\n",
        "    \"•\",\n",
        "    \"…\",\n",
        "    \"‰\",\n",
        "    \"′\",\n",
        "    \"″\",\n",
        "    \"⁄\",\n",
        "    \"⁎\",\n",
        "    \"⁶\",\n",
        "    \"⁹\",\n",
        "    \"₀\",\n",
        "    \"€\",\n",
        "    \"℃\",\n",
        "    \"ℜ\",\n",
        "    \"™\",\n",
        "    \"Ω\",\n",
        "    \"Ⅰ\",\n",
        "    \"Ⅱ\",\n",
        "    \"Ⅲ\",\n",
        "    \"→\",\n",
        "    \"↓\",\n",
        "    \"↵\",\n",
        "    \"⇑\",\n",
        "    \"⌬\",\n",
        "    \"⌿\",\n",
        "    \"⍀\",\n",
        "    \"␣\",\n",
        "    \"␤\",\n",
        "    \"␥\",\n",
        "    \"␦\",\n",
        "    \"■\",\n",
        "    \"▪\",\n",
        "    \"▶\",\n",
        "    \"▸\",\n",
        "    \"►\",\n",
        "    \"○\",\n",
        "    \"◗\",\n",
        "    \"★\",\n",
        "    \"☆\",\n",
        "    \"✔\",\n",
        "    \"✜\",\n",
        "    \"✩\",\n",
        "    \"➜\",\n",
        "    \"⩾\",\n",
        "    \"、\",\n",
        "    \"・\",\n",
        "    \"Ϳ\",\n",
        "    \"΄\",\n",
        "    \"·\",\n",
        "    \"Ί\",\n",
        "    \"Α\",\n",
        "    \"Γ\",\n",
        "    \"Ε\",\n",
        "    \"Θ\",\n",
        "    \"Ι\",\n",
        "    \"Λ\",\n",
        "    \"Μ\",\n",
        "    \"ϩ\",\n",
        "    \"Ϫ\",\n",
        "    \"ϫ\",\n",
        "    \"Ϭ\",\n",
        "    \"ϭ\",\n",
        "    \"Ϯ\",\n",
        "    \"ϯ\",\n",
        "    \"ϰ\",\n",
        "    \"ϱ\",\n",
        "    \"ϲ\",\n",
        "    \"ϳ\",\n",
        "    \"ϵ\",\n",
        "    \"Ϸ\",\n",
        "    \"Ͻ\",\n",
        "    \"Ͼ\",\n",
        "    \"Ј\",\n",
        "    \"Љ\",\n",
        "    \"Њ\",\n",
        "    \"А\",\n",
        "    \"Б\",\n",
        "    \"В\",\n",
        "    \"Д\",\n",
        "    \"И\",\n",
        "    \"К\",\n",
        "    \"Н\",\n",
        "    \"О\",\n",
        "    \"Р\",\n",
        "    \"С\",\n",
        "    \"Т\",\n",
        "    \"У\",\n",
        "    \"Ф\",\n",
        "    \"Х\",\n",
        "    \"Ц\",\n",
        "    \"Ч\",\n",
        "    \"Ш\",\n",
        "    \"中\",\n",
        "    \"乌\",\n",
        "    \"亏\",\n",
        "    \"代\",\n",
        "    \"何\",\n",
        "    \"充\",\n",
        "    \"冒\",\n",
        "    \"吃\",\n",
        "    \"國\",\n",
        "    \"型\",\n",
        "    \"子\",\n",
        "    \"學\",\n",
        "    \"寄\",\n",
        "    \"寒\",\n",
        "    \"山\",\n",
        "    \"感\",\n",
        "    \"扬\",\n",
        "    \"方\",\n",
        "    \"明\",\n",
        "    \"是\",\n",
        "    \"暑\",\n",
        "    \"替\",\n",
        "    \"板\",\n",
        "    \"根\",\n",
        "    \"桑\",\n",
        "    \"民\",\n",
        "    \"決\",\n",
        "    \"熱\",\n",
        "    \"狗\",\n",
        "    \"理\",\n",
        "    \"生\",\n",
        "    \"福\",\n",
        "    \"脊\",\n",
        "    \"膽\",\n",
        "    \"與\",\n",
        "    \"良\",\n",
        "    \"芳\",\n",
        "    \"藍\",\n",
        "    \"藥\",\n",
        "    \"處\",\n",
        "    \"補\",\n",
        "    \"論\",\n",
        "    \"醫\",\n",
        "    \"钟\",\n",
        "    \"間\",\n",
        "    \"風\",\n",
        "    \"首\",\n",
        "    \"龍\",\n",
        "    \"가\",\n",
        "    \"각\",\n",
        "    \"간\",\n",
        "    \"감\",\n",
        "    \"갑\",\n",
        "    \"강\",\n",
        "    \"같\",\n",
        "    \"개\",\n",
        "    \"객\",\n",
        "    \"거\",\n",
        "    \"걱\",\n",
        "    \"건\",\n",
        "    \"걸\",\n",
        "    \"검\",\n",
        "    \"것\",\n",
        "    \"게\",\n",
        "    \"겨\",\n",
        "    \"격\",\n",
        "    \"겪\",\n",
        "    \"결\",\n",
        "    \"겼\",\n",
        "    \"경\",\n",
        "    \"계\",\n",
        "    \"고\",\n",
        "    \"공\",\n",
        "    \"과\",\n",
        "    \"관\",\n",
        "    \"교\",\n",
        "    \"구\",\n",
        "    \"국\",\n",
        "    \"군\",\n",
        "    \"그\",\n",
        "    \"근\",\n",
        "    \"글\",\n",
        "    \"급\",\n",
        "    \"기\",\n",
        "    \"긴\",\n",
        "    \"길\",\n",
        "    \"까\",\n",
        "    \"꺼\",\n",
        "    \"꼈\",\n",
        "    \"나\",\n",
        "    \"낙\",\n",
        "    \"난\",\n",
        "    \"남\",\n",
        "    \"났\",\n",
        "    \"내\",\n",
        "    \"넷\",\n",
        "    \"년\",\n",
        "    \"노\",\n",
        "    \"높\",\n",
        "    \"누\",\n",
        "    \"느\",\n",
        "    \"는\",\n",
        "    \"능\",\n",
        "    \"니\",\n",
        "    \"다\",\n",
        "    \"단\",\n",
        "    \"달\",\n",
        "    \"당\",\n",
        "    \"대\",\n",
        "    \"던\",\n",
        "    \"도\",\n",
        "    \"동\",\n",
        "    \"되\",\n",
        "    \"된\",\n",
        "    \"두\",\n",
        "    \"드\",\n",
        "    \"든\",\n",
        "    \"들\",\n",
        "    \"등\",\n",
        "    \"따\",\n",
        "    \"때\",\n",
        "    \"또\",\n",
        "    \"라\",\n",
        "    \"람\",\n",
        "    \"램\",\n",
        "    \"략\",\n",
        "    \"량\",\n",
        "    \"러\",\n",
        "    \"렇\",\n",
        "    \"레\",\n",
        "    \"려\",\n",
        "    \"력\",\n",
        "    \"련\",\n",
        "    \"령\",\n",
        "    \"로\",\n",
        "    \"록\",\n",
        "    \"론\",\n",
        "    \"롯\",\n",
        "    \"료\",\n",
        "    \"루\",\n",
        "    \"률\",\n",
        "    \"르\",\n",
        "    \"른\",\n",
        "    \"를\",\n",
        "    \"리\",\n",
        "    \"립\",\n",
        "    \"마\",\n",
        "    \"만\",\n",
        "    \"말\",\n",
        "    \"망\",\n",
        "    \"매\",\n",
        "    \"머\",\n",
        "    \"멀\",\n",
        "    \"메\",\n",
        "    \"며\",\n",
        "    \"면\",\n",
        "    \"명\",\n",
        "    \"모\",\n",
        "    \"목\",\n",
        "    \"못\",\n",
        "    \"무\",\n",
        "    \"문\",\n",
        "    \"물\",\n",
        "    \"미\",\n",
        "    \"밀\",\n",
        "    \"및\",\n",
        "    \"바\",\n",
        "    \"반\",\n",
        "    \"받\",\n",
        "    \"발\",\n",
        "    \"방\",\n",
        "    \"배\",\n",
        "    \"백\",\n",
        "    \"번\",\n",
        "    \"범\",\n",
        "    \"법\",\n",
        "    \"별\",\n",
        "    \"병\",\n",
        "    \"보\",\n",
        "    \"복\",\n",
        "    \"본\",\n",
        "    \"부\",\n",
        "    \"분\",\n",
        "    \"불\",\n",
        "    \"비\",\n",
        "    \"빈\",\n",
        "    \"사\",\n",
        "    \"산\",\n",
        "    \"상\",\n",
        "    \"생\",\n",
        "    \"서\",\n",
        "    \"석\",\n",
        "    \"선\",\n",
        "    \"설\",\n",
        "    \"성\",\n",
        "    \"세\",\n",
        "    \"소\",\n",
        "    \"속\",\n",
        "    \"손\",\n",
        "    \"쇄\",\n",
        "    \"수\",\n",
        "    \"순\",\n",
        "    \"술\",\n",
        "    \"슈\",\n",
        "    \"스\",\n",
        "    \"시\",\n",
        "    \"식\",\n",
        "    \"신\",\n",
        "    \"실\",\n",
        "    \"심\",\n",
        "    \"써\",\n",
        "    \"아\",\n",
        "    \"악\",\n",
        "    \"안\",\n",
        "    \"않\",\n",
        "    \"알\",\n",
        "    \"았\",\n",
        "    \"애\",\n",
        "    \"야\",\n",
        "    \"약\",\n",
        "    \"양\",\n",
        "    \"어\",\n",
        "    \"언\",\n",
        "    \"얼\",\n",
        "    \"없\",\n",
        "    \"었\",\n",
        "    \"에\",\n",
        "    \"여\",\n",
        "    \"역\",\n",
        "    \"연\",\n",
        "    \"염\",\n",
        "    \"였\",\n",
        "    \"영\",\n",
        "    \"예\",\n",
        "    \"와\",\n",
        "    \"왔\",\n",
        "    \"외\",\n",
        "    \"요\",\n",
        "    \"욕\",\n",
        "    \"용\",\n",
        "    \"우\",\n",
        "    \"운\",\n",
        "    \"울\",\n",
        "    \"움\",\n",
        "    \"원\",\n",
        "    \"월\",\n",
        "    \"웠\",\n",
        "    \"위\",\n",
        "    \"유\",\n",
        "    \"육\",\n",
        "    \"율\",\n",
        "    \"으\",\n",
        "    \"은\",\n",
        "    \"을\",\n",
        "    \"음\",\n",
        "    \"응\",\n",
        "    \"의\",\n",
        "    \"이\",\n",
        "    \"인\",\n",
        "    \"일\",\n",
        "    \"임\",\n",
        "    \"입\",\n",
        "    \"있\",\n",
        "    \"자\",\n",
        "    \"작\",\n",
        "    \"잘\",\n",
        "    \"잠\",\n",
        "    \"장\",\n",
        "    \"재\",\n",
        "    \"저\",\n",
        "    \"적\",\n",
        "    \"전\",\n",
        "    \"절\",\n",
        "    \"점\",\n",
        "    \"접\",\n",
        "    \"정\",\n",
        "    \"제\",\n",
        "    \"조\",\n",
        "    \"족\",\n",
        "    \"존\",\n",
        "    \"종\",\n",
        "    \"주\",\n",
        "    \"준\",\n",
        "    \"줄\",\n",
        "    \"중\",\n",
        "    \"증\",\n",
        "    \"지\",\n",
        "    \"직\",\n",
        "    \"진\",\n",
        "    \"질\",\n",
        "    \"징\",\n",
        "    \"차\",\n",
        "    \"착\",\n",
        "    \"찰\",\n",
        "    \"참\",\n",
        "    \"처\",\n",
        "    \"척\",\n",
        "    \"철\",\n",
        "    \"첫\",\n",
        "    \"청\",\n",
        "    \"체\",\n",
        "    \"쳐\",\n",
        "    \"촉\",\n",
        "    \"총\",\n",
        "    \"최\",\n",
        "    \"추\",\n",
        "    \"축\",\n",
        "    \"출\",\n",
        "    \"충\",\n",
        "    \"취\",\n",
        "    \"측\",\n",
        "    \"치\",\n",
        "    \"칠\",\n",
        "    \"코\",\n",
        "    \"콩\",\n",
        "    \"크\",\n",
        "    \"타\",\n",
        "    \"태\",\n",
        "    \"택\",\n",
        "    \"터\",\n",
        "    \"토\",\n",
        "    \"통\",\n",
        "    \"트\",\n",
        "    \"특\",\n",
        "    \"파\",\n",
        "    \"판\",\n",
        "    \"퍼\",\n",
        "    \"편\",\n",
        "    \"평\",\n",
        "    \"폐\",\n",
        "    \"포\",\n",
        "    \"폭\",\n",
        "    \"푛\",\n",
        "    \"표\",\n",
        "    \"품\",\n",
        "    \"프\",\n",
        "    \"피\",\n",
        "    \"하\",\n",
        "    \"학\",\n",
        "    \"한\",\n",
        "    \"할\",\n",
        "    \"함\",\n",
        "    \"항\",\n",
        "    \"해\",\n",
        "    \"핵\",\n",
        "    \"했\",\n",
        "    \"행\",\n",
        "    \"향\",\n",
        "    \"헌\",\n",
        "    \"험\",\n",
        "    \"혀\",\n",
        "    \"현\",\n",
        "    \"형\",\n",
        "    \"호\",\n",
        "    \"혹\",\n",
        "    \"홍\",\n",
        "    \"화\",\n",
        "    \"확\",\n",
        "    \"환\",\n",
        "    \"활\",\n",
        "    \"황\",\n",
        "    \"회\",\n",
        "    \"효\",\n",
        "    \"후\",\n",
        "    \"휴\",\n",
        "    \"흡\",\n",
        "    \"\\u202a\",\n",
        "    \"\\u202b\",\n",
        "    \"\\u202c\",\n",
        "    \"\\ue024\",\n",
        "    \"\\ue02c\",\n",
        "    \"\\ue02e\",\n",
        "    \"\\ue031\",\n",
        "    \"\\ue032\",\n",
        "    \"\\ue033\",\n",
        "    \"\\ue035\",\n",
        "    \"\\ue061\",\n",
        "    \"\\ue062\",\n",
        "    \"\\ue06d\",\n",
        "    \"\\ue152\",\n",
        "    \"\\uf020\",\n",
        "    \"\\uf02b\",\n",
        "    \"\\uf02d\",\n",
        "    \"\\uf02f\",\n",
        "    \"\\uf03d\",\n",
        "    \"\\uf044\",\n",
        "    \"\\uf046\",\n",
        "    \"\\uf05b\",\n",
        "    \"\\uf05d\",\n",
        "    \"\\uf061\",\n",
        "    \"\\uf062\",\n",
        "    \"\\uf063\",\n",
        "    \"\\uf065\",\n",
        "    \"\\uf067\",\n",
        "    \"\\uf06b\",\n",
        "    \"\\uf06c\",\n",
        "    \"\\uf06d\",\n",
        "    \"\\uf09f\",\n",
        "    \"\\uf0a2\",\n",
        "    \"\\uf0a3\",\n",
        "    \"\\uf0a7\",\n",
        "    \"\\uf0ae\",\n",
        "    \"\\uf0b0\",\n",
        "    \"\\uf0b4\",\n",
        "    \"\\uf0b7\",\n",
        "    \"\\uf0bb\",\n",
        "    \"\\uf0d7\",\n",
        "    \"\\uf0e0\",\n",
        "    \"\\uf6d9\",\n",
        "    \"\\uf761\",\n",
        "    \"\\uf762\",\n",
        "    \"\\uf764\",\n",
        "    \"\\uf765\",\n",
        "    \"\\uf766\",\n",
        "    \"\\uf767\",\n",
        "    \"\\uf768\",\n",
        "    \"\\uf769\",\n",
        "    \"\\uf76b\",\n",
        "    \"\\uf76c\",\n",
        "    \"\\uf76e\",\n",
        "    \"\\uf76f\",\n",
        "    \"\\uf770\",\n",
        "    \"\\uf772\",\n",
        "    \"\\uf773\",\n",
        "    \"\\uf774\",\n",
        "    \"\\uf775\",\n",
        "    \"\\uf776\",\n",
        "    \"\\uf777\",\n",
        "    \"\\uf778\",\n",
        "    \"\\uf779\",\n",
        "    \"\\uf77a\",\n",
        "    \"�\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEU2LXywbo4b"
      },
      "source": [
        "### **Creación de la clase Cord19 para cargar los datos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qzIkvkWbnhb"
      },
      "source": [
        "class Cord19(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        type_path,\n",
        "        num_samples,\n",
        "        input_length,\n",
        "        output_length,\n",
        "        print_text=False,\n",
        "    ):\n",
        "        self.dataset = load_dataset(\n",
        "            \"cord19\", \"fulltext\", data_dir=\"data/\", split=type_path\n",
        "        )\n",
        "        if num_samples:\n",
        "            self.dataset = self.dataset.get(\"train\").select(list(range(0, num_samples)))\n",
        "        self.input_length = input_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.output_length = output_length\n",
        "        self.print_text = print_text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.translate({ord(x): \"\" for x in CHARS})\n",
        "        text = text.replace(\"\\n\", \"\")\n",
        "        text = text.replace(\"``\", \"\")\n",
        "        text = text.replace('\"', \"\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    def convert_to_features(self, example_batch):\n",
        "        # Tokenize contexts and questions (as pairs of inputs)\n",
        "\n",
        "        if self.print_text:\n",
        "            print(\"Input Text: \", self.clean_text(example_batch[\"fulltext\"]))\n",
        "\n",
        "        input_ = self.clean_text(example_batch[\"fulltext\"])\n",
        "        target_ = self.clean_text(example_batch[\"abstract\"])\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [input_],\n",
        "            max_length=self.input_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        targets = self.tokenizer.batch_encode_plus(\n",
        "            [target_],\n",
        "            max_length=self.output_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return source, targets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        source, targets = self.convert_to_features(self.dataset[index])\n",
        "\n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        target_ids = targets[\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask = source[\"attention_mask\"].squeeze()\n",
        "        target_mask = targets[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"source_ids\": source_ids,\n",
        "            \"source_mask\": src_mask,\n",
        "            \"target_ids\": target_ids,\n",
        "            \"target_mask\": target_mask,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMpvDA4fcMoo"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "dataset = Cord19(tokenizer, 'train', None, 512, 150, True)\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti3Ld5xsdnvX"
      },
      "source": [
        "tokenizer.batch_encode_plus([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnayM9E6c3uh"
      },
      "source": [
        "data = dataset[50]\n",
        "print()\n",
        "print(\"Shape of Tokenized Text: \", data['source_ids'].shape)\n",
        "print()\n",
        "print(\"Sanity check - Decode Text: \", tokenizer.decode(data['source_ids']))\n",
        "print(\"====================================\")\n",
        "print(\"Sanity check - Decode Summary: \", tokenizer.decode(data['target_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}